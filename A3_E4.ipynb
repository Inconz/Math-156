{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Load Wisconsin Breast Cancer dataset from scikit-learnâ€™s built-in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Split the dataset into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the data to train + val and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#splits the train + val set into train and val \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Report the size of each class in your training (+ validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 has 169 samples\n",
      "Class 1 has 286 samples\n"
     ]
    }
   ],
   "source": [
    "#counts the number of the sample from the training + validation set\n",
    "class_0_counter = len(y_train_val[y_train_val == 0])\n",
    "class_1_counter = len(y_train_val[y_train_val == 1])\n",
    "print(\"Class 0 has\", class_0_counter, \"samples\")\n",
    "print(\"Class 1 has\", class_1_counter, \"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (d) Train a binary logistic regression model using your implementation from problem 3. \n",
    " Initialize the model weights randomly, sampling from a standard Gaussian distribution. \n",
    " Experiment with different choices of fixed learning rate and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyunc\\AppData\\Local\\Temp\\ipykernel_13456\\969293360.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, dim):\n",
    "        #initializes the logistic regression model with the numbers of features in the dataset.\n",
    "        self.w = np.random.randn(dim)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        #outputs probilities after applying it to logistic sigmoid function.\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #predicts the data points class by the threshold of 0.5.\n",
    "        probabilities = self.sigmoid(np.dot(X, self.w))\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "    \n",
    "    def cross_entropy_error(self, X, t):\n",
    "        #computes the cross_entropy_error\n",
    "        y = self.predict(X, return_proba=True)\n",
    "        error = -np.sum(t * np.log(y) + (1 - t) * np.log(1 - y))\n",
    "        return error\n",
    "    \n",
    "class MiniBatchSGD:\n",
    "    def __init__(self, model, batch_size=64, learning_rate=0.001, max_iterations=10000):\n",
    "        #initializes the mini-batch SGD parameters\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations \n",
    "    \n",
    "    def random_select_batch(self, X, t):\n",
    "        #randomly selects a batch\n",
    "        i = np.random.choice(X.shape[0], self.batch_size, replace=False)\n",
    "        X_batch = X[i]\n",
    "        t_batch = t[i]\n",
    "        return X_batch, t_batch\n",
    "        \n",
    "    def iterate(self, X, t):\n",
    "     for i in range(self.max_iterations):\n",
    "            #select a random mini-batch\n",
    "            X_batch, t_batch = self.random_select_batch(X, t)\n",
    "            \n",
    "            #predicts the probability of the batch\n",
    "            z_batch = np.dot(X_batch, self.model.w)\n",
    "            y_batch = self.model.sigmoid(z_batch)\n",
    "            \n",
    "            #computes the gradient of the selcted batch\n",
    "            gradient = np.dot(X_batch.T, (y_batch - t_batch)) / self.batch_size\n",
    "            \n",
    "            #update the model's weights\n",
    "            self.model.w -= self.learning_rate * gradient\n",
    "            \n",
    "            return self.model\n",
    "     \n",
    "#initialize the model with feature dimension\n",
    "model = LogisticRegression(dim=X_train.shape[1])\n",
    "\n",
    "#train the model with different learning rates and batch sizes\n",
    "optimizer = MiniBatchSGD(model=model, batch_size=32, learning_rate=0.005, max_iterations=1000)\n",
    "trained_model = optimizer.iterate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Use the trained model to report the performance of the model on the test set. For evaluation\n",
    " metrics, use accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6228070175438597\n",
      "Precision: 0.6228070175438597\n",
      "Recall: 1.0\n",
      "F1-Score: 0.7675675675675676\n"
     ]
    }
   ],
   "source": [
    "y_pred = trained_model.predict(X_test)\n",
    "\n",
    "#computes accuracy, precision, recall, and f1-score of the prediction\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "f1_score = f1_score(y_test, y_pred, zero_division=1)\n",
    "\n",
    "#prints each evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-score shows the balance between the precision and recall. The Recall of 1.0 suggest that model is identifying all the benign tumor(Class 0, negative) correctly. However, precision of 62.3% suggests that many predictions is identifying benign tumor as maligant tumor(Class 1, positive) which is a Type I Error. In this case, we prefer having higher recall than precision as identifying maliganant tumor as benign tumor could potentially worsen patient's condition. Lastly, low accuracy of 62.3% indicates that the model struggles to make correct predictions for about one-third of the cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
